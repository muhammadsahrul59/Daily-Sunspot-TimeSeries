# -*- coding: utf-8 -*-
"""ProyekTimeSeries.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lsJSeY0SFygPUIEMq62yucieukHPJKnE

# **ProyekTimeSeries**

Nama : Muhammad Sahrul

Email : m.sahrul59@gmail.com

ID Dicoding : sahrul57
"""

! pip install -q kaggle
from google.colab import files
files.upload()

! mkdir ~/.kaggle

! cp kaggle.json ~/.kaggle/

! chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d patrickfleith/daily-sunspots-dataset

!unzip daily-sunspots-dataset.zip

import numpy as np
import pandas as pd
from keras.layers import Dense, LSTM
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from sklearn.preprocessing import StandardScaler

df = pd.read_csv('/content/daily_sunspots_time_series_1850_2023.csv')
df

df.info()

df.isnull().sum()

df = df.drop(columns=['date', 'indicator'])
df.head()

scaler = StandardScaler()
scaler.fit(df)
scaled_data = scaler.transform(df)
scaled_data = pd.DataFrame(scaled_data, columns=df.columns)
scaled_data.head()

series = scaled_data['counts'].values
time_step = scaled_data['year'].values

series

plt.figure(figsize=(14,7))
plt.plot(time_step, series)
plt.xlabel('Year')
plt.ylabel('Sunspot Counts')
plt.title('Sunspot Counts Over Years',
          fontsize=15)

# Plotting the distribution of sunspot counts over the years
plt.figure(figsize=(14, 7))

# Line plot for sunspot counts over the years
sns.lineplot(data=df, x='year', y='counts', errorbar=None)

plt.title('Sunspot Counts Over the Years')
plt.xlabel('Year')
plt.ylabel('Sunspot Counts')
plt.show()

# Plotting the distribution of sunspot counts
plt.figure(figsize=(14, 6))

# Creating a histogram
plt.subplot(1, 2, 1)
sns.histplot(df['counts'], bins=50, kde=True)
plt.title('Histogram of Sunspot Counts')
plt.xlabel('Sunspot Counts')
plt.ylabel('Frequency')

# Creating a boxplot
plt.subplot(1, 2, 2)
sns.boxplot(x=df['counts'])
plt.title('Boxplot of Sunspot Counts')
plt.xlabel('Sunspot Counts')

# Display the plots
plt.tight_layout()
plt.show()

def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis=-1)
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[-1:]))
    return ds.batch(batch_size).prefetch(1)

from sklearn.model_selection import train_test_split

time_train, x_train, time_valid, x_valid = train_test_split(series,
                                                            time_step,
                                                            test_size=0.2,
                                                            shuffle=False)

window_size = 60
batch_size = 100
shuffle_buffer_size = 1000

train_set = windowed_dataset(x_train,
                             window_size=window_size,
                             batch_size=batch_size,
                             shuffle_buffer=shuffle_buffer_size)

class ThresholdedMAECallback(tf.keras.callbacks.Callback):
    def __init__(self , threshold):
        super(ThresholdedMAECallback, self).__init__()
        self.threshold = threshold

    def on_epoch_end(self, epoch, logs=None):
        current_mae = logs.get('mae')
        if current_mae < self.threshold:
            print(f"\nMAE has reached the threshold of ({self.threshold}), training is stopped.")
            self.model.stop_training = True

threshold_mae = (scaled_data['counts'].max() - scaled_data['counts'].min()) * 10/100
mae_callback = ThresholdedMAECallback(threshold_mae)

model = tf.keras.models.Sequential([
  tf.keras.layers.Conv1D(filters=60, kernel_size=5,
                        strides=1, padding='causal',
                        activation='relu', input_shape=[None, 1]),
  tf.keras.layers.LSTM(60, return_sequences=True),
  tf.keras.layers.LSTM(60),
  tf.keras.layers.Dense(30, activation='relu'),
  tf.keras.layers.Dense(10, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(1)
])

model.summary()

optimizer = tf.keras.optimizers.SGD(lr=1e-5, momentum=0.9)

model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])

history = model.fit(train_set, epochs=100, callbacks=[mae_callback])

loss=history.history['loss']
epochs=range(len(loss))

plt.plot(epochs, loss, 'r')
plt.title('Training loss')
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend(["Loss"])

plt.figure()